{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> XOKind - Machine Learning/Data Science Intern Interview <center>\n",
    "## <center> Yelp rating predictions <center>\n",
    "### <center> Traditional Machine learning Vs Graph Machine Learning <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Machine Learning - Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate confusion matrix images from confusion matrix array\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(title+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data files\n",
    "\n",
    "business_json_path = 'dataset/business.json'\n",
    "review_json_path = 'dataset/review.json'\n",
    "user_json_path = 'dataset/user.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read business file and extract restaurant data\n",
    "\n",
    "size = 500000\n",
    "\n",
    "business = pd.read_json(business_json_path, lines=True,\n",
    "                    dtype={'business_id':str,'name':str,\n",
    "                             'address':str,'city':str,\n",
    "                             'latitude':float,'longitude':float,\n",
    "                             'state':str,'postal_code':str,\n",
    "                             'stars':float,'review_count':int,\n",
    "                             'is_open':int,\n",
    "                             'attributes':object,'categories':object,\n",
    "                             'hours':object},\n",
    "                    chunksize=size)\n",
    "\n",
    "\n",
    "business_drop_columns = ['name', 'address', 'city', 'state', 'postal_code',\n",
    "                         'latitude', 'longitude', 'attributes', 'hours']\n",
    "chunk_list_business = []\n",
    "\n",
    "for chunk_business in business:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_business = chunk_business.drop(business_drop_columns, axis=1)\n",
    "    \n",
    "    # Renaming column name to avoid conflicts\n",
    "    chunk_business.rename(columns={'stars': 'business_stars', 'review_count': 'business_review_count',\n",
    "                                      'review_stars': 'business_review_stars'}, inplace=True)\n",
    "    \n",
    "    chunk_business = chunk_business[chunk_business['categories'].str.contains('Restaurants', case=True,na=False)]\n",
    "    \n",
    "    chunk_list_business.append(chunk_business)\n",
    "\n",
    "    \n",
    "df_restaurants = pd.concat(chunk_list_business, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del chunk_business\n",
    "del chunk_list_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Reviews data\n",
    "# =============================================================================\n",
    "size = 500000\n",
    "\n",
    "review = pd.read_json(review_json_path, lines=True,\n",
    "                      dtype={'review_id':str,'user_id':str,\n",
    "                             'business_id':str,'stars':int,\n",
    "                             'date':str,'text':str,'useful':int,\n",
    "                             'funny':int,'cool':int},\n",
    "                      chunksize=size)\n",
    "\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    \n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['text', 'date', 'review_id','useful','funny','cool'], axis=1)\n",
    "    \n",
    "    # Renaming column name to avoid conflicts\n",
    "    chunk_review = chunk_review.rename(columns={'stars': 'review_stars'})\n",
    "    \n",
    "    # Inner merge with edited business file so only reviews related to the restaurants remain\n",
    "    chunk_merged = pd.merge(df_restaurants, chunk_review, on='business_id', how='inner')\n",
    "    \n",
    "    # Show feedback on progress\n",
    "    print(f\"{chunk_merged.shape[0]} out of {size:,} related reviews\")\n",
    "    \n",
    "    chunk_list.append(chunk_merged)\n",
    "\n",
    "    \n",
    "# After trimming down the review file, concatenate all relevant data back to one dataframe\n",
    "df_restaurant_reviews = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del chunk_review\n",
    "del chunk_merged\n",
    "del chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# User data\n",
    "# =============================================================================\n",
    "\n",
    "size = 500000\n",
    "\n",
    "user = pd.read_json(user_json_path, lines=True,\n",
    "                      dtype={'user_id':str,'name':str,\n",
    "                             'yelping_since':str,'review_count':int,\n",
    "                             'friends':object,'useful':int,\n",
    "                             'funny':int,'cool':int,'fans':int,\n",
    "                             'elite':list, 'average_stars':float,'compliment_hot':int,\n",
    "                             'compliment_more':int,'compliment_more':int,'compliment_profile':int,\n",
    "                             'compliment_cute':int,'compliment_list':int,'compliment_note':int,\n",
    "                             'compliment_plain':int,'compliment_cool':int,'compliment_funny':int,\n",
    "                             'compliment_writer':int,'compliment_photos':int},\n",
    "                      chunksize=size)\n",
    "\n",
    "user_drop_columns = ['name', 'yelping_since', 'friends']\n",
    "\n",
    "chunk_list_user = []\n",
    "\n",
    "for chunk_user in user:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_user = chunk_user.drop(user_drop_columns, axis=1)\n",
    "    \n",
    "    # Renaming column name to avoid conflicts\n",
    "    chunk_user.rename(columns={'review_count': 'user_review_count', 'average_stars': 'user_average_stars'})\n",
    "    \n",
    "    chunk_list_user.append(chunk_user)\n",
    "\n",
    "    \n",
    "# concatenate to one dataframe\n",
    "df_user = pd.concat(chunk_list_user, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del chunk_user\n",
    "del chunk_list_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge users and restaurant reviews data --> this dataframe will contain information about user, restaurant and review\n",
    "\n",
    "merged_df = df_user.merge(df_restaurant_reviews, how='inner', left_on=[\"user_id\"], right_on=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del df_user\n",
    "del df_restaurant_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature mean compliment score for each users\n",
    "\n",
    "merged_drop_columns = ['business_id', 'user_id', 'elite']\n",
    "\n",
    "merged_df.drop(merged_drop_columns, axis = 1, inplace = True)\n",
    "\n",
    "compliment_columns = ['compliment_cool', 'compliment_cute', 'compliment_funny', \n",
    "                               'compliment_hot', 'compliment_list', 'compliment_more',\n",
    "                               'compliment_note', 'compliment_photos', 'compliment_plain', \n",
    "                               'compliment_profile', 'compliment_writer']\n",
    "\n",
    "\n",
    "merged_df['mean_compliment_score'] = merged_df.loc[: , compliment_columns].mean(axis=1)\n",
    "\n",
    "\n",
    "merged_df = merged_df.drop(compliment_columns, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand by restaurant category to investigate restaurent categories and their overall count in data\n",
    "\n",
    "df_yelp_expand_by_category = merged_df.assign(categories = merged_df.categories\n",
    "                         .str.split(', ')).explode('categories')\n",
    "df_yelp_category_count = df_yelp_expand_by_category.categories.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting top 10 restaurants based on count\n",
    "top_10_restaurants = list(df_yelp_category_count.index.values)[1:11] #first element is Resturant, so index 1 to 11\n",
    "\n",
    "\n",
    "df_yelp_top10 = df_yelp_expand_by_category.loc[df_yelp_expand_by_category['categories'].isin(top_10_restaurants)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One Hot Encoding for categories column - Not needed for Random forest\n",
    "\n",
    "#df_yelp_top10_ohe = pd.get_dummies(data = df_yelp_top10, prefix = 'is', \n",
    "#                                                     columns = ['categories'], drop_first= True, sparse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del df_yelp_expand_by_category\n",
    "del merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize columns - (x- mean(x))/std(x) --> Not needed for Tree based algorithms such as Random Forests\n",
    "\n",
    "#cols_to_norm = ['review_count','useful', 'funny', 'cool', 'fans', 'average_stars', 'business_stars', \n",
    "#                'business_review_count', 'mean_compliment_score']\n",
    "\n",
    "#df_yelp_top10_ohe[cols_to_norm] = df_yelp_top10_ohe[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and testing sets for model training\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_yelp_top10.drop(['review_stars'], axis = 1).values, \n",
    "                                                    df_yelp_top10[['review_stars']].values, test_size=0.2, \n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categories using label encoder - no need for one hot encoding for random forest model\n",
    "encoder_x = LabelEncoder()\n",
    "encoder_x.fit(x_train[:,9])\n",
    "\n",
    "x_train[:,9] = encoder_x.transform(x_train[:,9])\n",
    "x_test[:,9] =  encoder_x.transform(x_test[:,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding for target variable\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest model definition - we can input multiple hyperparameters and the rf_grid will save model with \n",
    "#hyper-parameters that gave best validation accuracy\n",
    "\n",
    "print('Random Forest Classification')\n",
    "\n",
    "params_rf = {'n_estimators' : [50], 'max_depth': [20]}\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_jobs = -1, criterion='gini', \n",
    "                       min_samples_leaf=10, \n",
    "                       oob_score = True, \n",
    "                       class_weight = 'balanced', \n",
    "                       random_state=0)\n",
    "\n",
    "\n",
    "rf_grid = GridSearchCV(rf_model, params_rf, \n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model training\n",
    "rf_grid.fit(x_train, encoded_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "\n",
    "rf_test_predictions = rf_grid.best_estimator_.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To print classification report with metrics such as accuracy, precision, recall and f1-score\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "\n",
    "print(classification_report(y_test_encoded, rf_test_predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print confusion matrix\n",
    "cm = confusion_matrix(y_test_encoded, rf_test_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, target_names, normalize = False, title = 'CM_RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, target_names, normalize = True, title = 'CM_RF_normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> END <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
