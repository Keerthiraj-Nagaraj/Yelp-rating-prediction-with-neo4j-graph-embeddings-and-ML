{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> XOKind - Machine Learning/Data Science Intern Interview <center>\n",
    "## <center> Yelp rating predictions <center>\n",
    "### <center> Traditional Machine learning Vs Graph Machine Learning <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Machine Learning - Multilayer Perceptron Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate confusion matrix images from confusion matrix array\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(title+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data files\n",
    "\n",
    "business_json_path = 'dataset/business.json'\n",
    "review_json_path = 'dataset/review.json'\n",
    "user_json_path = 'dataset/user.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read business file and extract restaurant data\n",
    "\n",
    "size = 500000\n",
    "\n",
    "business = pd.read_json(business_json_path, lines=True,\n",
    "                    dtype={'business_id':str,'name':str,\n",
    "                             'address':str,'city':str,\n",
    "                             'latitude':float,'longitude':float,\n",
    "                             'state':str,'postal_code':str,\n",
    "                             'stars':float,'review_count':int,\n",
    "                             'is_open':int,\n",
    "                             'attributes':object,'categories':object,\n",
    "                             'hours':object},\n",
    "                    chunksize=size)\n",
    "\n",
    "\n",
    "business_drop_columns = ['name', 'address', 'city', 'state', 'postal_code',\n",
    "                         'latitude', 'longitude', 'attributes', 'hours']\n",
    "chunk_list_business = []\n",
    "\n",
    "for chunk_business in business:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_business = chunk_business.drop(business_drop_columns, axis=1)\n",
    "    \n",
    "    # Renaming column name to avoid conflicts\n",
    "    chunk_business.rename(columns={'stars': 'business_stars', 'review_count': 'business_review_count',\n",
    "                                      'review_stars': 'business_review_stars'}, inplace=True)\n",
    "    \n",
    "    chunk_business = chunk_business[chunk_business['categories'].str.contains('Restaurants', case=True,na=False)]\n",
    "    \n",
    "    chunk_list_business.append(chunk_business)\n",
    "\n",
    "    \n",
    "df_restaurants = pd.concat(chunk_list_business, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del chunk_business\n",
    "del chunk_list_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Reviews data\n",
    "# =============================================================================\n",
    "size = 500000\n",
    "\n",
    "review = pd.read_json(review_json_path, lines=True,\n",
    "                      dtype={'review_id':str,'user_id':str,\n",
    "                             'business_id':str,'stars':int,\n",
    "                             'date':str,'text':str,'useful':int,\n",
    "                             'funny':int,'cool':int},\n",
    "                      chunksize=size)\n",
    "\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    \n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['text', 'date', 'review_id','useful','funny','cool'], axis=1)\n",
    "    \n",
    "    # Renaming column name to avoid conflicts\n",
    "    chunk_review = chunk_review.rename(columns={'stars': 'review_stars'})\n",
    "    \n",
    "    # Inner merge with edited business file so only reviews related to the restaurants remain\n",
    "    chunk_merged = pd.merge(df_restaurants, chunk_review, on='business_id', how='inner')\n",
    "    \n",
    "    # Show feedback on progress\n",
    "    print(f\"{chunk_merged.shape[0]} out of {size:,} related reviews\")\n",
    "    \n",
    "    chunk_list.append(chunk_merged)\n",
    "\n",
    "    \n",
    "# After trimming down the review file, concatenate all relevant data back to one dataframe\n",
    "df_restaurant_reviews = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del chunk_review\n",
    "del chunk_merged\n",
    "del chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# User data\n",
    "# =============================================================================\n",
    "\n",
    "size = 500000\n",
    "\n",
    "user = pd.read_json(user_json_path, lines=True,\n",
    "                      dtype={'user_id':str,'name':str,\n",
    "                             'yelping_since':str,'review_count':int,\n",
    "                             'friends':object,'useful':int,\n",
    "                             'funny':int,'cool':int,'fans':int,\n",
    "                             'elite':list, 'average_stars':float,'compliment_hot':int,\n",
    "                             'compliment_more':int,'compliment_more':int,'compliment_profile':int,\n",
    "                             'compliment_cute':int,'compliment_list':int,'compliment_note':int,\n",
    "                             'compliment_plain':int,'compliment_cool':int,'compliment_funny':int,\n",
    "                             'compliment_writer':int,'compliment_photos':int},\n",
    "                      chunksize=size)\n",
    "\n",
    "user_drop_columns = ['name', 'yelping_since', 'friends']\n",
    "\n",
    "chunk_list_user = []\n",
    "\n",
    "for chunk_user in user:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_user = chunk_user.drop(user_drop_columns, axis=1)\n",
    "    \n",
    "    # Renaming column name to avoid conflicts\n",
    "    chunk_user.rename(columns={'review_count': 'user_review_count', 'average_stars': 'user_average_stars'})\n",
    "    \n",
    "    chunk_list_user.append(chunk_user)\n",
    "\n",
    "    \n",
    "# concatenate to one dataframe\n",
    "df_user = pd.concat(chunk_list_user, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del chunk_user\n",
    "del chunk_list_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge users and restaurant reviews data --> this dataframe will contain information about user, restaurant and review\n",
    "\n",
    "merged_df = df_user.merge(df_restaurant_reviews, how='inner', left_on=[\"user_id\"], right_on=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del df_user\n",
    "del df_restaurant_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature mean compliment score for each users\n",
    "\n",
    "merged_drop_columns = ['business_id', 'user_id', 'elite']\n",
    "\n",
    "merged_df.drop(merged_drop_columns, axis = 1, inplace = True)\n",
    "\n",
    "compliment_columns = ['compliment_cool', 'compliment_cute', 'compliment_funny', \n",
    "                               'compliment_hot', 'compliment_list', 'compliment_more',\n",
    "                               'compliment_note', 'compliment_photos', 'compliment_plain', \n",
    "                               'compliment_profile', 'compliment_writer']\n",
    "\n",
    "\n",
    "merged_df['mean_compliment_score'] = merged_df.loc[: , compliment_columns].mean(axis=1)\n",
    "\n",
    "\n",
    "merged_df.drop(compliment_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand by restaurant category to investigate restaurent categories and their overall count in data\n",
    "\n",
    "df_yelp_expand_by_category = merged_df.assign(categories = df_yelp.categories\n",
    "                         .str.split(', ')).explode('categories')\n",
    "df_yelp_category_count = df_yelp_expand_by_category.categories.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting top 10 restaurants based on count\n",
    "top_10_restaurants = list(df_yelp_category_count.index.values)[1:11] #first element is Resturant, so index 1 to 11\n",
    "\n",
    "\n",
    "df_yelp_top10 = df_yelp_expand_by_category.loc[df_yelp_expand_by_category['categories'].isin(top_10_restaurants)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One Hot Encoding for categories column\n",
    "\n",
    "df_yelp_top10_ohe = pd.get_dummies(data = df_yelp_top10, prefix = 'is', \n",
    "                                                     columns = ['categories'], drop_first= True, sparse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete non-essential data to save memory\n",
    "\n",
    "del df_yelp_expand_by_category\n",
    "del merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize columns - (x- mean(x))/std(x) --> important for Gradient Descent based algorithms such as MLPNN\n",
    "\n",
    "cols_to_norm = ['review_count','useful', 'funny', 'cool', 'fans', 'average_stars', 'business_stars', \n",
    "                'business_review_count', 'mean_compliment_score']\n",
    "\n",
    "df_yelp_top10_ohe[cols_to_norm] = df_yelp_top10_ohe[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and testing sets for model training\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_yelp_top10_ohe.drop(['review_stars'], axis = 1).values, \n",
    "                                                    df_yelp_top10_ohe[['review_stars']].values, test_size=0.2, \n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting from object type to float\n",
    "\n",
    "x_train = x_train.astype(float)\n",
    "x_test = x_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding and One hot Encoding for target variable\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(encoded_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate class weights = to address imbalanced class size\n",
    "\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "counts = counts/sum(counts)\n",
    "\n",
    "inv_counts = 1/counts\n",
    "\n",
    "class_weights = {}\n",
    "\n",
    "for i in range(len(unique)):\n",
    "    class_weights[i] = inv_counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Configure a simple MLPNN model with many of the default parameters.\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(40, input_dim=x_train.shape[1], activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training and saving history of training and validation accuracies\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    batch_size=1000,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing targets in the test data for performance comparison + Predicting ratings for test data\n",
    "\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "y_test_ohe = np_utils.to_categorical(y_test_encoded)\n",
    "\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "y_pred += 1 #to match the labels as model outputs values 0-4 instead of 1-5 which label encoder uses\n",
    "\n",
    "\n",
    "y_pred_encoded = encoder.transform(y_pred)\n",
    "y_pred_ohe = np_utils.to_categorical(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To print classification report with metrics such as accuracy, precision, recall and f1-score\n",
    "\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, target_names, normalize = False, title = 'CM_MLPNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, target_names, normalize = True, title = 'CM_MLPNN_normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> END <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
